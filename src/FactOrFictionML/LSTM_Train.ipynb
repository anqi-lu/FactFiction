{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cntk as C\n",
    "import numpy as np\n",
    "import copy\n",
    "import cntk.tests.test_utils\n",
    "cntk.tests.test_utils.set_device_from_pytest_env() # (only needed for our build system)\n",
    "C.cntk_py.set_fixed_random_seed(1) # fix a random seed for CNTK components\n",
    "\n",
    "azureml_share_env = 'AZUREML_NATIVE_SHARE_DIRECTORY'\n",
    "is_azure_ml = azureml_share_env in os.environ\n",
    "share_path = os.environ[azureml_share_env] if is_azure_ml else '../../'\n",
    "\n",
    "train_path = os.path.join(share_path, \"data/final/final.train.ctf\")\n",
    "val_path = os.path.join(share_path, \"data/final/final.val.ctf\")\n",
    "test_path = os.path.join(share_path, \"data/final/final.test.ctf\")\n",
    "\n",
    "C.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the reader\n",
    "def create_reader(path, is_training, input_dim, label_dim):\n",
    "    return C.io.MinibatchSource(C.io.CTFDeserializer(path, C.io.StreamDefs(\n",
    "        features = C.io.StreamDef(field='S0', shape=input_dim,   is_sparse=True),\n",
    "        labels   = C.io.StreamDef(field='S1', shape=label_dim,   is_sparse=False)\n",
    "    )), randomize=is_training, max_sweeps = C.io.INFINITELY_REPEAT if is_training else 1)\n",
    "\n",
    "# Defines the LSTM model for classifying sequences\n",
    "def lstm_sequence_classifier(features, num_classes, embedding_dim, LSTM_dim):\n",
    "    with C.layers.default_options(initial_state=0.1):\n",
    "        classifier = C.layers.Sequential([C.layers.Embedding(embedding_dim, name='embed'),\n",
    "                                          C.layers.Recurrence(C.layers.LSTM(LSTM_dim), go_backwards=False),\n",
    "                                          C.sequence.last,\n",
    "                                          C.layers.Dense(num_classes, name='dense')])\n",
    "    return classifier(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the model and criterion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 101590\n",
    "\n",
    "# Model dimensions\n",
    "input_dim = vocab_size\n",
    "hidden_dim = 500\n",
    "embedding_dim = 1000\n",
    "num_classes = 3\n",
    "\n",
    "# Input variables denoting the features and label data\n",
    "features = C.sequence.input_variable(shape=input_dim, is_sparse=True)\n",
    "labels = C.input_variable(num_classes)\n",
    "\n",
    "# Instantiate the sequence classification model\n",
    "model = lstm_sequence_classifier(features, num_classes, embedding_dim, hidden_dim)\n",
    "\n",
    "# Create criterion\n",
    "loss        = C.cross_entropy_with_softmax(model, labels)\n",
    "label_error = C.classification_error(model, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the reader for training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = create_reader(train_path, True, input_dim, num_classes)\n",
    "print(reader.streams.keys())\n",
    "\n",
    "reader_val = create_reader(val_path, True, input_dim, num_classes)\n",
    "print(reader_val.streams.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_epochs = 10\n",
    "\n",
    "epoch_size = 9206 # Total number of sequences\n",
    "minibatch_size = 300 # Minimum number of tokens being fetched in a minibatch\n",
    "\n",
    "epoch_size_val = 1150\n",
    "\n",
    "progress_printer = C.logging.ProgressPrinter(minibatch_size)\n",
    "\n",
    "# SGD learner\n",
    "#lr_per_sample = C.learners.learning_rate_schedule(0.0015, C.learners.UnitType.sample)\n",
    "# learner = C.learners.sgd(model.parameters, lr=lr_per_sample)\n",
    "\n",
    "lr_schedule = C.learning_parameter_schedule(1, minibatch_size=C.learners.IGNORE)\n",
    "t_schedule = C.momentum_schedule(0.971, minibatch_size=C.learners.IGNORE)\n",
    "learner = adadelta = C.adadelta(model.parameters, lr_schedule, 0.999, 1e-6)\n",
    "\n",
    "trainer = C.Trainer(model, (loss, label_error),\n",
    "                    learner,\n",
    "                    progress_printer)\n",
    "\n",
    "input_map = {\n",
    "    features : reader.streams.features,\n",
    "    labels   : reader.streams.labels\n",
    "}\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    # Train on one epoch\n",
    "    t = 0\n",
    "    while t < epoch_size:\n",
    "        mb = reader.next_minibatch(minibatch_size, input_map=input_map)\n",
    "        trainer.train_minibatch(mb)\n",
    "        t += mb[labels].num_samples # Current number of read sequences\n",
    "    trainer.summarize_training_progress()\n",
    "    \n",
    "    print('Evaluating on the validation set')\n",
    "    \n",
    "    # Evaluate validation set after one epoch\n",
    "    t = 0\n",
    "    while t < epoch_size_val:\n",
    "        mb = reader_val.next_minibatch(minibatch_size, input_map=input_map)\n",
    "        trainer.test_minibatch(mb)\n",
    "        t += mb[labels].num_samples\n",
    "    trainer.summarize_test_progress()\n",
    "    \n",
    "    print('End of epoch', epoch)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader_test = create_reader(test_path, False, input_dim, num_classes)\n",
    "\n",
    "num_test_sequences = 1150\n",
    "test_minibatch_size = 1000\n",
    "test_result = 0.0\n",
    "\n",
    "read_test_sequences = 0\n",
    "while read_test_sequences < num_test_sequences:\n",
    "    mb = reader_test.next_minibatch(test_minibatch_size, input_map=input_map)\n",
    "    eval_error = trainer.test_minibatch(mb)\n",
    "    test_result = test_result + eval_error\n",
    "    read_test_sequences += mb[labels].num_samples\n",
    "\n",
    "trainer.summarize_test_progress()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'outputs/lstm_model.cmf'\n",
    "model.save(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess.normalize_sentences import SentenceNormalizer\n",
    "from cntk.ops.functions import load_model\n",
    "\n",
    "saved_model = load_model(model_path)\n",
    "\n",
    "with open('dictionary.txt', 'r', encoding='utf-8') as f:\n",
    "    dictionary = f.read().strip().split('\\n') \n",
    "\n",
    "sent_normalizer = SentenceNormalizer(dictionary=dictionary)\n",
    "normalized = sent_normalizer.fit_transform(\n",
    "    [\"Mortgage payoff trick eliminates up to 15 years\",\n",
    "     \"Mortgage payoff trick eliminates up to\"], to_index=True)\n",
    "\n",
    "print(normalized)\n",
    "pred_score = saved_model(C.Value.one_hot(normalized, vocab_size))\n",
    "print(pred_score)\n",
    "\n",
    "pred_class = np.argmax(pred_score, axis=1)\n",
    "labels = []\n",
    "with open('labels.txt', 'r', encoding='utf-8') as f:\n",
    "    labels = f.read().strip().split('\\n')\n",
    "pred_class = [labels[p] for p in pred_class]\n",
    "print(pred_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
